{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cfc23963-2cc5-4f1d-8278-fb1b2026afc5",
      "metadata": {
        "id": "cfc23963-2cc5-4f1d-8278-fb1b2026afc5"
      },
      "source": [
        "## Assignment: $k$ Means Clustering\n",
        "\n",
        "Tamera Fang (ven7sg)\n",
        "\n",
        "## **Do two questions.**\n",
        "\n",
        "`! git clone https://www.github.com/DS3001/kmc`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85dae156-b49b-4f43-bc42-ad1589132891",
      "metadata": {
        "id": "85dae156-b49b-4f43-bc42-ad1589132891"
      },
      "source": [
        "**Q1.** This question is a case study for $k$ means clustering.\n",
        "\n",
        "1. Load the `airbnb_hw.csv` data. Clean `Price` along with `Beds`, `Number of Reviews`, and `Review Scores Rating`.\n",
        "\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/Users/tamerafang/PycharmProjects/DS3001_kMC/airbnb_hw.csv')\n",
        "\n",
        "df['price'] = df['Price']\n",
        "df['beds'] = df['Beds']\n",
        "df['n_reviews'] = df['Number Of Reviews']\n",
        "df['score'] = df['Review Scores Rating']\n",
        "cleaned_data = df.loc[:,['price','beds','n_reviews','score']]\n",
        "print(cleaned_data.shape)\n",
        "print(cleaned_data.describe())\n",
        "\n",
        "cleaned_data['price'].value_counts()\n",
        "cleaned_data['price'] = cleaned_data['price'].str.replace(',','')\n",
        "cleaned_data['price'] = pd.to_numeric(cleaned_data['price'],errors='coerce')\n",
        "print(cleaned_data.describe())\n",
        "\n",
        "cleaned_data['beds'] = cleaned_data['beds'].fillna(1)\n",
        "print(cleaned_data.describe())\n",
        "\n",
        "pd.crosstab(df['score'].isnull(), df['n_reviews']>0)\n",
        "cleaned_data = cleaned_data.dropna()\n",
        "print(cleaned_data.describe())\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "2. Maxmin normalize the data and remove any `nan`'s (`KMeans` from `sklearn` doesn't accept `nan` input).\n",
        "\n",
        "\n",
        "```\n",
        "def maxmin(x):\n",
        "    u = (x-min(x))/(max(x)-min(x))\n",
        "    return u\n",
        "normalized = cleaned_data.drop('price',axis=1)\n",
        "normalized = normalized.apply(maxmin)\n",
        "```\n",
        "\n",
        "\n",
        "3. Use `sklearn`'s `KMeans` module to cluster the data by `Beds`, `Number of Reviews`, and `Review Scores Rating` for `k=6`.\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.cluster import KMeans\n",
        "model = KMeans(n_clusters=6, max_iter=300, n_init = 10, random_state=0)\n",
        "model = model.fit(normalized) # Fit the emodel\n",
        "normalized['cluster'] = model.labels_\n",
        "print(normalized.describe())\n",
        "```\n",
        "\n",
        "\n",
        "4. Use `seaborn`'s `.pairplot()` to make a grid of scatterplots that show how the clustering is carried out in multiple dimensions.\n",
        "\n",
        "\n",
        "```\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "pairplot = sns.pairplot(normalized, hue='cluster', palette='bright', vars=['beds', 'n_reviews', 'score'])\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "5. Use `.groupby` and `.describe` to compute the average price for each cluster. Which clusters have the highest rental prices?\n",
        "\n",
        "* The cluster with the highest rental price is cluster 3 around $293.53 per night.\n",
        "```\n",
        "cleaned_data['cluster'] = model.labels_\n",
        "avg_cluster_price = cleaned_data.loc[:,['price','cluster']].groupby('cluster').describe()\n",
        "print(avg_cluster_price)\n",
        "mean_prices = avg_cluster_price['price']['mean']\n",
        "highest_price_cluster = mean_prices.idxmax()\n",
        "print(f\"Highest average price cluster: {highest_price_cluster}\")\n",
        "print(f\"Average price for this cluster: ${mean_prices[highest_price_cluster]:.2f}\")\n",
        "```\n",
        "\n",
        "\n",
        "6. Use a scree plot to pick the number of clusters and repeat steps 4 and 5.\n",
        "\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "\n",
        "k_bar = 15\n",
        "k_grid = np.arange(1, k_bar + 1)\n",
        "SSE = np.zeros(k_bar)\n",
        "for k in range(k_bar):\n",
        "    model = KMeans(n_clusters=k+1, max_iter=300, n_init=10, random_state=0)\n",
        "    model.fit(normalized)\n",
        "    SSE[k] = model.inertia_\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x=k_grid, y=SSE)\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('SSE')\n",
        "plt.show()\n",
        "\n",
        "chosen_clusters = 3  \n",
        "\n",
        "model = KMeans(n_clusters=chosen_clusters, max_iter=300, n_init=10, random_state=0)\n",
        "model.fit(normalized)\n",
        "normalized['cluster'] = model.labels_\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.pairplot(data=normalized, hue='cluster', palette='bright')\n",
        "plt.show()\n",
        "cleaned_data['cluster'] = model.labels_\n",
        "cluster_price_stats = cleaned_data.loc[:, ['price', 'cluster']].groupby('cluster').describe()\n",
        "print(cluster_price_stats)\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26523935-9e8f-4377-920c-6f65605c0e31",
      "metadata": {
        "id": "26523935-9e8f-4377-920c-6f65605c0e31"
      },
      "source": [
        "**Q2.** This is a question about $k$ means clustering. We want to investigate how adjusting the \"noisiness\" of the data impacts the quality of the algorithm and the difficulty of picking $k$.\n",
        "\n",
        "1. Run the code below, which creates four datasets: `df0_125`, `df0_25`, `df0_5`, `df1_0`, and `df2_0`. Each data set is created by increasing the amount of `noise` (standard deviation) around the cluster centers, from `0.125` to `0.25` to `0.5` to `1.0` to `2.0`.\n",
        "* ran code below\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def createData(noise,N=50):\n",
        "    np.random.seed(100) # Set the seed for replicability\n",
        "    # Generate (x1,x2,g) triples:\n",
        "    X1 = np.array([np.random.normal(1,noise,N),np.random.normal(1,noise,N)])\n",
        "    X2 = np.array([np.random.normal(3,noise,N),np.random.normal(2,noise,N)])\n",
        "    X3 = np.array([np.random.normal(5,noise,N),np.random.normal(3,noise,N)])\n",
        "    # Concatenate into one data frame\n",
        "    gdf1 = pd.DataFrame({'x1':X1[0,:],'x2':X1[1,:],'group':'a'})\n",
        "    gdf2 = pd.DataFrame({'x1':X2[0,:],'x2':X2[1,:],'group':'b'})\n",
        "    gdf3 = pd.DataFrame({'x1':X3[0,:],'x2':X3[1,:],'group':'c'})\n",
        "    df = pd.concat([gdf1,gdf2,gdf3],axis=0)\n",
        "    return df\n",
        "\n",
        "df0_125 = createData(0.125)\n",
        "df0_25 = createData(0.25)\n",
        "df0_5 = createData(0.5)\n",
        "df1_0 = createData(1.0)\n",
        "df2_0 = createData(2.0)\n",
        "```\n",
        "\n",
        "2. Make scatterplots of the $(X1,X2)$ points by group for each of the datasets. As the `noise` goes up from 0.125 to 2.0, what happens to the visual distinctness of the clusters?\n",
        "\n",
        "* As noise increases, we see that the clusters become more spread out. This causes the visual distinctness to decrease as the clusters collide and are on top of each other.\n",
        "\n",
        "\n",
        "```\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "def scatter(df, noise_level, ax):\n",
        "    sns.scatterplot(data=df, x='x1', y='x2', hue='group', style='group', ax=ax)\n",
        "    ax.set_title(f'Scatterplot with Noise Level: {noise_level}')\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(10.5, 7))\n",
        "\n",
        "scatter(df0_125, '0.125', axes[0, 0])\n",
        "scatter(df0_25, '0.25', axes[0, 1])\n",
        "scatter(df0_5, '0.5', axes[0, 2])\n",
        "scatter(df1_0, '1.0', axes[1, 0])\n",
        "scatter(df2_0, '2.0', axes[1, 1])\n",
        "\n",
        "axes[1, 2].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. Create a scree plot for each of the datasets. Describe how the level of `noise` affects the scree plot (particularly the presence of a clear \"elbow\") and your ability to definitively select a $k$.\n",
        "* As noise increases, the clarity of the elbow decreases. It becomes harder to select an elbow since it isn't as clear.\n",
        "\n",
        "```\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def maxmin(x):\n",
        "    x = (x-min(x))/(max(x)-min(x))\n",
        "    return x\n",
        "\n",
        "def scree(data, ax, title):\n",
        "    X = data.loc[:, ['x1', 'x2']]\n",
        "    X = X.apply(maxmin)\n",
        "    k_bar = 15\n",
        "    k_grid = np.arange(1, k_bar + 1)\n",
        "    SSE = np.zeros(k_bar)\n",
        "    for k in range(k_bar):\n",
        "        model = KMeans(n_clusters=k + 1, max_iter=300, n_init=10, random_state=0)\n",
        "        model.fit(X)\n",
        "        SSE[k] = model.inertia_\n",
        "    sns.lineplot(x=k_grid, y=SSE, ax=ax)\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylim(0, 35)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(10.5, 7))\n",
        "scree(df0_125, axes[0, 0], 'Noise 0.125')\n",
        "scree(df0_25, axes[0, 1], 'Noise 0.25')\n",
        "scree(df0_5, axes[0, 2], 'Noise 0.5')\n",
        "scree(df1_0, axes[1, 0], 'Noise 1.0')\n",
        "scree(df2_0, axes[1, 1], 'Noise 2.0')\n",
        "axes[1, 2].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "4. Explain the intuition of the elbow, using this numerical simulation as an example.\n",
        "* In this numerical simulation, we can see that the noise varies at 5 levels: 0.125, 0.25, 0.5, 1.0, and 2.0. As the noise changes, we notice how the difference in cluster distinctness affects the appearance of the elbows in our scree plots.  At higher noise levels, the clusters are most spread out with more overlap and are harder to distinguish; at low levels, the clusters are closer together and distinct. The marginal benefit diminished with more clusters. This also means that as SSE decreases, K increases.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "870041b4-204b-4bc4-bf8a-3fe2f8734a58",
      "metadata": {
        "id": "870041b4-204b-4bc4-bf8a-3fe2f8734a58"
      },
      "source": [
        "**Q3.** We looked at computer vision with $k$NN in a previous question. Can $k$ means clustering correctly group digits, even if we don't know which symbols are which?\n",
        "\n",
        "1. To load the data, run the following code in a chunk:\n",
        "```\n",
        "from keras.datasets import mnist\n",
        "df = mnist.load_data('minst.db')\n",
        "train,test = df\n",
        "X_train, y_train = train\n",
        "X_test, y_test = test\n",
        "```\n",
        "The `y_test` and `y_train` vectors, for each index `i`, tell you want number is written in the corresponding index in `X_train[i]` and `X_test[i]`. The value of `X_train[i]` and `X_test[i]`, however, is a 28$\\times$28 array whose entries contain values between 0 and 256. Each element of the matrix is essentially a \"pixel\" and the matrix encodes a representation of a number. To visualize this, run the following code to see the first ten numbers:\n",
        "```\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.set_printoptions(edgeitems=30, linewidth=100000)\n",
        "for i in range(5):\n",
        "    print(y_test[i],'\\n') # Print the label\n",
        "    print(X_test[i],'\\n') # Print the matrix of values\n",
        "    plt.contourf(np.rot90(X_test[i].transpose())) # Make a contour plot of the matrix values\n",
        "    plt.show()\n",
        "```\n",
        "OK, those are the data: Labels attached to handwritten digits encoded as a matrix.\n",
        "\n",
        "2. What is the shape of `X_train` and `X_test`? What is the shape of `X_train[i]` and `X_test[i]` for each index `i`? What is the shape of `y_train` and `y_test`?\n",
        "3. Use Numpy's `.reshape()` method to covert the training and testing data from a matrix into an vector of features. So, `X_test[index].reshape((1,784))` will convert the $index$-th element of `X_test` into a $28\\times 28=784$-length row vector of values, rather than a matrix. Turn `X_train` into an $N \\times 784$ matrix $X$ that is suitable for scikit-learn's kNN classifier where $N$ is the number of observations and $784=28*28$ (you could use, for example, a `for` loop).\n",
        "4. Use $k$ means clustering on the reshaped `X_test` data with `k=10`.  \n",
        "5. Cross tabulate the cluster assignments with the true labels for the test set values. How good is the correspondence? What proportion of digits are clustered correctly? Which digits are the hardest to distinguish from one another? Can $k$MC recover the latent digits 0 to 9, without even knowing what those digits were?\n",
        "6. If you use a scree plot to determine the number of clusters $k$, does it pick 10 (the true number of digits), or not? If it fails to pick $k=10$, which digits does it tend to combine into the same classification?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}